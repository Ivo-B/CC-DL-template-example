# @package _global_

# to execute this experiment run:
# python run.py experiment=example_full.yaml

defaults:
  - override /trainer: null # override trainer to null so it's not loaded from main config defaults...
  - override /model: null
  - override /datamodule: null
  - override /callbacks: null
  - override /logger: null

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

seed: "OxCAFFEE"

trainer:
  loss:
    _target_: tensorflow.keras.losses.CategoricalCrossentropy
    from_logits: True
  metric:
    _target_: tensorflow.keras.metrics.CategoricalAccuracy
  optimizer:
    _target_: tensorflow.keras.callbacks.ReduceLROnPlateau
    factor: 0.5
    patience: 10
    min_lr: 1e-9
    verbose: 1
    monitor: 'val_loss'
    mode: 'min'
  lr_scheduler:
    _target_: tensorflow.keras.optimizers.Adam
    learning_rate: 0.0003
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-07
    amsgrad: 'false'

  _target_: cctest.models.base_trainer_module.TrainingModule
  # set `-1` to train on all GPUs in a node,
  # '>0' to train on specific num of GPUs in a node,
  # `0` to train on CPU only
  gpus: -1
  epochs: 5
  # resume_from_checkpoint: ${work_dir}/last.ckpt

model:
  _target_: cctest.models.modules.simple_conv_net.SimpleConvNet
  input_shape: [28, 28, 1]
  conv1_size: 16
  conv2_size: 32
  conv3_size: 32
  conv4_size: 64
  output_size: 10

datamodule:
  _target_: cctest.datamodule.mnist_datamodule.MNISTDataset
  data_dir: ${data_dir} # data_dir is specified in config.yaml
  data_training_list: 'training_data.txt'
  data_val_list: 'validation_data.txt'
  data_test_list: 'test_data.txt'
  batch_size: 64

callbacks:
  model_checkpoint:
    _target_: tensorflow.keras.callbacks.ModelCheckpoint
    monitor: 'val_loss' # name of the logged metric which determines when model is improving
    mode: 'min' # can be 'max' or 'min'
    save_best_only: True # save best model (determined by above metric)
    save_freq: 'epoch' # 'epoch' or integer. When using 'epoch', the callback saves the model after each epoch. When using integer, the callback saves the model at end of this many batches.
    verbose: 0
    filepath: 'checkpoints/epoch_{epoch:03d}-{val_loss:.2f}.tf'
    save_format: 'tf'
  early_stopping:
    _target_: tensorflow.keras.callbacks.EarlyStopping
    monitor: 'val_loss' # name of the logged metric which determines when model is improving
    mode: 'max' # can be 'max' or 'min'
    patience: 100 # how many validation epochs of not improving until training stops
    min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement

logger:
  tensorboard:
    _target_: tensorflow.keras.callbacks.TensorBoard
    log_dir: 'tensorboard/${name}'
    write_graph: False
    profile_batch: 0
  csv:
    _target_: tensorflow.keras.callbacks.CSVLogger
    filename: './csv/${name}.csv'
